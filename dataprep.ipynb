{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959556ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.preprocessing import normalize, StandardScaler, OneHotEncoder, PowerTransformer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "763d629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "course = pd.read_csv('Course_info.csv')\n",
    "course = course[course['language'].isin({'English', 'Indonesian'})].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f5fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trimming Outliers\n",
    "\n",
    "'''\n",
    "def outliers_handling(data, features, alpha=0.1):\n",
    "  outliers_indices = set()\n",
    "\n",
    "  for col in features:\n",
    "    upper = data[col].quantile(1-alpha)\n",
    "    lower = data[col].quantile(alpha)\n",
    "\n",
    "    outside = data[(data[col] < lower) | (data[col] > upper)]\n",
    "    outliers_indices.update(outside.index)\n",
    "\n",
    "  trim = data.drop(index=outliers_indices)\n",
    "  log_trim = trim.copy()\n",
    "  log_trim[features] = np.log1p(trim[features])\n",
    "\n",
    "  return trim, log_trim\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f379a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes(data, shift='avg_rating'):\n",
    "  categorical = []\n",
    "  numerical = []\n",
    "\n",
    "  for i, cat in enumerate(data.select_dtypes(include = ['object', 'bool']).columns.values):\n",
    "    categorical.append(cat)\n",
    "  categorical.append(shift)\n",
    "\n",
    "  for i, num in enumerate(data.select_dtypes(include = 'number').drop(columns='id').columns.values):\n",
    "    if num != shift:\n",
    "      numerical.append(num)\n",
    "\n",
    "  return categorical, numerical\n",
    "\n",
    "\n",
    "def data_cleaning(data, features, par=0.9):\n",
    "  outliers_indices = set()\n",
    "\n",
    "  for col in features:\n",
    "    exclude = data[col].quantile(par)\n",
    "    outliers = data[data[col] > exclude]\n",
    "    outliers_indices.update(outliers.index)\n",
    "    \n",
    "  trim = data.drop(index=outliers_indices)\n",
    "  \n",
    "  pt = PowerTransformer(method='yeo-johnson')\n",
    "  transformed = trim.copy()\n",
    "  transformed[features] = pt.fit_transform(transformed[features])\n",
    "\n",
    "  return trim, transformed\n",
    "\n",
    "\n",
    "def features_type(data):\n",
    "  return {\n",
    "      'semantic': ['title', 'headline'],\n",
    "      'nominal': ['is_paid', 'category', 'subcategory'],\n",
    "      'datetime': ['published_time', 'last_update_date'],\n",
    "      'high_cardinal': 'instructor_name',\n",
    "      'ordinal': 'avg_rating'}\n",
    "\n",
    "\n",
    "def calc_smoothed_instructor_rating(data, feature, rating='avg_rating', subscriber='num_subscribers', weight=50):\n",
    "  data['engagement'] = data[rating] * data[subscriber]\n",
    "\n",
    "  instructor_stats = data.groupby(feature).agg(\n",
    "      total_rating=('engagement', 'sum'),\n",
    "      total_subs=(subscriber, 'sum'))\n",
    "\n",
    "  instructor_stats['weighted_avg'] = instructor_stats['total_rating'] / instructor_stats['total_subs']\n",
    "  global_avg = data['engagement'].sum() / data[subscriber].sum()\n",
    "  instructor_stats['smoothed'] = (\n",
    "      (instructor_stats['total_subs'] * instructor_stats['weighted_avg'] + weight * global_avg) /\n",
    "      (instructor_stats['total_subs'] + weight))\n",
    "\n",
    "  data['instructor_score'] = data[feature].map(instructor_stats['smoothed'])\n",
    "  data[['avg_rating', 'instructor_score']] = data[['avg_rating', 'instructor_score']].astype('int64')\n",
    "\n",
    "  return data[['avg_rating', 'instructor_score']]\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88a62793",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical, numerical = attributes(course)\n",
    "course_clean, course_clean_scaled = data_cleaning(course, numerical)\n",
    "types = features_type(course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec082c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_preprocessing(data, features, n_neighbors=10):\n",
    "  text = data.copy()\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  stemmer = PorterStemmer()\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "  english_features = text[features].apply(lambda col: col.apply(lambda text: text.lower()))\n",
    "\n",
    "  for col in features:\n",
    "    english_features[col] = english_features[col].apply(lambda text: nltk.word_tokenize(text))\n",
    "    english_features[col] = english_features[col].apply(lambda text: [word for word in text if word.lower() not in stop_words])\n",
    "    english_features[col] = english_features[col].apply(lambda text: [stemmer.stem(word) for word in text])\n",
    "    english_features[col] = english_features[col].apply(lambda text: ' '.join(text))\n",
    "\n",
    "  combined_text = english_features.apply(lambda row: ' '.join(row), axis=1)\n",
    "  vectorizer = TfidfVectorizer(max_features=5000, min_df=3, max_df=0.85, ngram_range=(1,2), use_idf=True, smooth_idf=True)\n",
    "  tfidf_matrix = vectorizer.fit_transform(combined_text)\n",
    "\n",
    "  knn = NearestNeighbors(n_neighbors=n_neighbors+1, metric='cosine')\n",
    "  knn.fit(tfidf_matrix)\n",
    "  distances, indices = knn.kneighbors(tfidf_matrix)\n",
    "  cosine_similarities = 1 - distances\n",
    "\n",
    "  return cosine_similarities[:, 1:], indices[:, 1:]\n",
    "\n",
    "\n",
    "def numerical_preprocessing(data, features, n_neighbors=10):\n",
    "  normalized_data = normalize(data[features])\n",
    "\n",
    "  knn = NearestNeighbors(n_neighbors=n_neighbors+1, metric='euclidean')\n",
    "  knn.fit(normalized_data)\n",
    "  distances, indices = knn.kneighbors(normalized_data)\n",
    "  euclidean_similarities = 1 - distances\n",
    "\n",
    "  return euclidean_similarities[:, 1:], indices[:, 1:]\n",
    "\n",
    "\n",
    "def nominal_preprocessing(data, features, n_neighbors=10):\n",
    "  data = data[features].copy()\n",
    "  categorical = pd.concat([data['is_paid'].astype('uint8'), \n",
    "                           pd.get_dummies(data['category'], prefix='category', dtype='uint8'), \n",
    "                           pd.get_dummies(data['subcategory'], prefix='sub_category', dtype='uint8')],\n",
    "                           axis=1)\n",
    "\n",
    "  pca_result = PCA(n_components=0.95).fit_transform(categorical)\n",
    "  knn = NearestNeighbors(n_neighbors=n_neighbors+1, metric='cosine')\n",
    "  knn.fit(pca_result)\n",
    "  distances, indices = knn.kneighbors(pca_result)\n",
    "  cosine_similarities = 1 - distances\n",
    "\n",
    "  return cosine_similarities[:, 1:], indices[:, 1:]\n",
    "\n",
    "\n",
    "def ordinal_preprocessing(data, n_neighbors=10):\n",
    "  ordinal_data = data.rank(axis=0, method='average')\n",
    "\n",
    "  normalized_data = normalize(ordinal_data, norm='l2', axis=1)\n",
    "  nbrs = NearestNeighbors(n_neighbors=n_neighbors+1, metric='cosine', n_jobs=-1)\n",
    "  nbrs.fit(normalized_data)\n",
    "\n",
    "  distances, indices = nbrs.kneighbors(normalized_data)\n",
    "  cosine_similarities = 1 - distances\n",
    "\n",
    "  return cosine_similarities[:, 1:], indices[:, 1:]\n",
    "\n",
    "\n",
    "def datetime_preprocessing(data, features, due='2022-10-10', decay=0.01, weights=None):\n",
    "  data = data[features].copy()\n",
    "\n",
    "  for col in features:\n",
    "    data[col] = pd.to_datetime(data[col]).dt.tz_localize(None)\n",
    "    col_name = f\"{col}_duration\"\n",
    "    data[col_name] = (pd.to_datetime(due) - data[col]).dt.days\n",
    "    data[col_name] = data[col_name].apply(lambda x: np.exp(-decay * x))\n",
    "\n",
    "  decay_cols = [f\"{col}_duration\" for col in features]\n",
    "  date_matrix = data[decay_cols].values\n",
    "\n",
    "  if weights:\n",
    "      for i, col in enumerate(decay_cols):\n",
    "          date_matrix[:, i] *= weights.get(col, 1.0)\n",
    "\n",
    "  model = NearestNeighbors(n_neighbors=10, metric='cosine')\n",
    "  model.fit(date_matrix)\n",
    "  distances, indices = model.kneighbors(date_matrix)\n",
    "  cosine_similarities = 1 - distances\n",
    "\n",
    "  return cosine_similarities[:, 1:], indices[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2b57e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_semantic, i_semantic = semantic_preprocessing(course_clean, types['semantic'])\n",
    "G_nominal, i_nominal = nominal_preprocessing(course_clean, types['nominal'])\n",
    "G_numeric, i_numeric = numerical_preprocessing(course_clean_scaled, features=numerical)\n",
    "ordinal_mod = calc_smoothed_instructor_rating(course_clean_scaled, types['high_cardinal'])\n",
    "G_ordinal, i_ordinal = ordinal_preprocessing(ordinal_mod)\n",
    "G_datetime, i_datetime= datetime_preprocessing(course_clean, types['datetime'])\n",
    "\n",
    "with open('preprocessed_data.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'G_semantic': G_semantic,\n",
    "        'i_semantic': i_semantic,\n",
    "        'G_nominal': G_nominal,\n",
    "        'i_nominal': i_nominal,\n",
    "        'G_numeric': G_numeric,\n",
    "        'i_numeric': i_numeric,\n",
    "        'ordinal_mod': ordinal_mod,\n",
    "        'G_ordinal': G_ordinal,\n",
    "        'i_ordinal': i_ordinal,\n",
    "        'G_datetime': G_datetime,\n",
    "        'i_datetime': i_datetime\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c441904",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preprocessed_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "#similarities\n",
    "G_semantic = data['G_semantic']\n",
    "G_nominal = data['G_nominal']\n",
    "G_numeric = data['G_numeric']\n",
    "G_ordinal = data['G_ordinal']\n",
    "G_datetime = data['G_datetime']\n",
    "\n",
    "#indices \n",
    "i_semantic = data['i_semantic']\n",
    "i_nominal = data['i_nominal']\n",
    "i_numeric = data['i_numeric']\n",
    "i_ordinal = data['i_ordinal']\n",
    "i_datetime = data['i_datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05553241",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_mod = calc_smoothed_instructor_rating(course_clean_scaled, types['high_cardinal'])\n",
    "all_features = {\n",
    "    'semantic': (G_semantic, i_semantic),\n",
    "    'nominal': (G_nominal, i_nominal),\n",
    "    'ordinal': (G_numeric, i_numeric),\n",
    "    'datetime': (G_ordinal, i_ordinal),\n",
    "    'numeric': (G_datetime, i_datetime)\n",
    "}\n",
    "\n",
    "weights = {\n",
    "    'semantic': 0.4,\n",
    "    'nominal': 0.3,\n",
    "    'ordinal': 0.15,\n",
    "    'datetime': 0.10,\n",
    "    'numeric': 0.05\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6be0ffc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of final similarity matrix: (91566, 91566)\n",
      "Number of non-zero elements: 7157198\n"
     ]
    }
   ],
   "source": [
    "def aggregate_similarities(all_features, weights, num_items=len(course_clean), n_neighbors=10):\n",
    "    combined_scores = {}\n",
    "\n",
    "    for feature_type, (sim_matrix, idx_matrix) in all_features.items():\n",
    "        weight = weights.get(feature_type, 0)\n",
    "\n",
    "        for i in range(num_items):\n",
    "            for j in range(idx_matrix.shape[1]):\n",
    "                neighbor_idx = idx_matrix[i, j]\n",
    "                similarity_score = sim_matrix[i, j]\n",
    "                \n",
    "                key1 = (min(i, neighbor_idx), max(i, neighbor_idx))\n",
    "                \n",
    "                combined_scores[key1] = combined_scores.get(key1, 0.0) + (similarity_score * weight)\n",
    "\n",
    "    rows, cols, data = [], [], []\n",
    "    for (r, c), val in combined_scores.items():\n",
    "        rows.append(r)\n",
    "        cols.append(c)\n",
    "        data.append(val)\n",
    "\n",
    "        if r != c:\n",
    "            rows.append(c)\n",
    "            cols.append(r)\n",
    "            data.append(val)\n",
    "\n",
    "    final_similarity_matrix = csr_matrix((data, (rows, cols)), shape=(num_items, num_items))\n",
    "\n",
    "    return final_similarity_matrix\n",
    "\n",
    "\n",
    "final_sim_matrix = aggregate_similarities(all_features, weights)\n",
    "\n",
    "print(f\"Shape of final similarity matrix: {final_sim_matrix.shape}\")\n",
    "print(f\"Number of non-zero elements: {final_sim_matrix.nnz}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ed12da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_item_similarity_graph(similarity_matrix, threshold=0.5, top_n_edges_per_node=10):\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for i, item_id in enumerate(course_clean['id']):\n",
    "        G.add_node(i, item_id=item_id)\n",
    "\n",
    "    num_items = similarity_matrix.shape[0]\n",
    "\n",
    "    for i in range(num_items):\n",
    "        row_sims = similarity_matrix.getrow(i).toarray().flatten()\n",
    "        candidate_indices = np.where(row_sims > 0)[0] \n",
    "        candidate_indices = candidate_indices[candidate_indices != i] \n",
    "        \n",
    "        if len(candidate_indices) == 0:\n",
    "            continue\n",
    "\n",
    "        sorted_indices = candidate_indices[np.argsort(-row_sims[candidate_indices])]\n",
    "        \n",
    "        if top_n_edges_per_node is not None:\n",
    "            edges_to_add = sorted_indices[:top_n_edges_per_node]\n",
    "        else:\n",
    "            edges_to_add = sorted_indices \n",
    "\n",
    "        for j in edges_to_add:\n",
    "            sim_score = row_sims[j]\n",
    "            if sim_score > threshold:\n",
    "                if not G.has_edge(i, j):\n",
    "                    G.add_edge(i, j, weight=sim_score)\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe38843e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in graph: 91566\n",
      "Number of edges in graph: 106749\n"
     ]
    }
   ],
   "source": [
    "G = build_item_similarity_graph(final_sim_matrix)\n",
    "\n",
    "print(f\"Number of nodes in graph: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges in graph: {G.number_of_edges()}\")\n",
    "\n",
    "with open('network_resource.pkl', 'wb') as g:\n",
    "    pickle.dump({'G': G}, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a9e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import community.community_louvain as community_louvain\n",
    "\n",
    "partition = community_louvain.best_partition(G)\n",
    "num_communities = len(set(partition.values()))\n",
    "print(f\"Number of detected communities: {num_communities}\")\n",
    "\n",
    "modularity = community_louvain.modularity(partition, G)\n",
    "print(f\"Modularity: {modularity:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
