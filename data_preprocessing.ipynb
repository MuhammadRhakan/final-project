{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "959556ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.preprocessing import normalize, StandardScaler, OneHotEncoder, PowerTransformer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "763d629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "course = pd.read_csv('Course_info.csv')\n",
    "course = course[course['language'].isin({'English', 'Indonesian'})].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f5fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trimming Outliers\n",
    "\n",
    "'''\n",
    "def outliers_handling(data, features, alpha=0.1):\n",
    "  outliers_indices = set()\n",
    "\n",
    "  for col in features:\n",
    "    upper = data[col].quantile(1-alpha)\n",
    "    lower = data[col].quantile(alpha)\n",
    "\n",
    "    outside = data[(data[col] < lower) | (data[col] > upper)]\n",
    "    outliers_indices.update(outside.index)\n",
    "\n",
    "  trim = data.drop(index=outliers_indices)\n",
    "  log_trim = trim.copy()\n",
    "  log_trim[features] = np.log1p(trim[features])\n",
    "\n",
    "  return trim, log_trim\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f379a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes(data, shift='avg_rating'):\n",
    "  categorical = []\n",
    "  numerical = []\n",
    "\n",
    "  for i, cat in enumerate(data.select_dtypes(include = ['object', 'bool']).columns.values):\n",
    "    categorical.append(cat)\n",
    "  categorical.append(shift)\n",
    "\n",
    "  for i, num in enumerate(data.select_dtypes(include = 'number').drop(columns='id').columns.values):\n",
    "    if num != shift:\n",
    "      numerical.append(num)\n",
    "\n",
    "  return categorical, numerical\n",
    "\n",
    "\n",
    "def data_cleaning(data, features, par=0.9):\n",
    "  outliers_indices = set()\n",
    "\n",
    "  for col in features:\n",
    "    exclude = data[col].quantile(par)\n",
    "    outliers = data[data[col] > exclude]\n",
    "    outliers_indices.update(outliers.index)\n",
    "    \n",
    "  trim = data.drop(index=outliers_indices)\n",
    "  \n",
    "  pt = PowerTransformer(method='yeo-johnson')\n",
    "  transformed = trim.copy()\n",
    "  transformed[features] = pt.fit_transform(transformed[features])\n",
    "\n",
    "  return trim, transformed\n",
    "\n",
    "\n",
    "def features_type(data):\n",
    "  return {\n",
    "      'semantic': ['title', 'headline'],\n",
    "      'nominal': ['is_paid', 'category', 'subcategory'],\n",
    "      'datetime': ['published_time', 'last_update_date'],\n",
    "      'high_cardinal': 'instructor_name',\n",
    "      'ordinal': 'avg_rating'}\n",
    "\n",
    "\n",
    "def calc_smoothed_instructor_rating(data, feature, rating='avg_rating', subscriber='num_subscribers', weight=50):\n",
    "  data['engagement'] = data[rating] * data[subscriber]\n",
    "\n",
    "  instructor_stats = data.groupby(feature).agg(\n",
    "      total_rating=('engagement', 'sum'),\n",
    "      total_subs=(subscriber, 'sum'))\n",
    "\n",
    "  instructor_stats['weighted_avg'] = instructor_stats['total_rating'] / instructor_stats['total_subs']\n",
    "  global_avg = data['engagement'].sum() / data[subscriber].sum()\n",
    "  instructor_stats['smoothed'] = (\n",
    "      (instructor_stats['total_subs'] * instructor_stats['weighted_avg'] + weight * global_avg) /\n",
    "      (instructor_stats['total_subs'] + weight))\n",
    "\n",
    "  data['instructor_score'] = data[feature].map(instructor_stats['smoothed'])\n",
    "  data.loc[(data[rating] == 0) | (data[subscriber] == 0), 'instructor_score'] = 0\n",
    "  data[['avg_rating', 'instructor_score']] = data[['avg_rating', 'instructor_score']].astype('int64')\n",
    "\n",
    "  return data[['avg_rating', 'instructor_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88a62793",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical, numerical = attributes(course)\n",
    "course_clean, course_clean_scaled = data_cleaning(course, numerical)\n",
    "types = features_type(course_clean)\n",
    "ordinal_mod = calc_smoothed_instructor_rating(course_clean, types['high_cardinal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ec082c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_preprocessing(data, features, n_neighbors=10):\n",
    "  text = data.copy()\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  stemmer = PorterStemmer()\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "  english_features = text[features].apply(lambda col: col.apply(lambda text: text.lower()))\n",
    "\n",
    "  for col in features:\n",
    "    english_features[col] = english_features[col].apply(lambda text: nltk.word_tokenize(text))\n",
    "    english_features[col] = english_features[col].apply(lambda text: [word for word in text if word.lower() not in stop_words])\n",
    "    english_features[col] = english_features[col].apply(lambda text: [stemmer.stem(word) for word in text])\n",
    "    english_features[col] = english_features[col].apply(lambda text: ' '.join(text))\n",
    "\n",
    "  combined_text = english_features.apply(lambda row: ' '.join(row), axis=1)\n",
    "  vectorizer = TfidfVectorizer(max_features=5000, min_df=3, max_df=0.85, ngram_range=(1,2), use_idf=True, smooth_idf=True)\n",
    "  tfidf_matrix = vectorizer.fit_transform(combined_text)\n",
    "\n",
    "  knn = NearestNeighbors(n_neighbors=n_neighbors+1, metric='cosine')\n",
    "  knn.fit(tfidf_matrix)\n",
    "  distances, indices = knn.kneighbors(tfidf_matrix)\n",
    "  cosine_similarities = 1 - distances\n",
    "\n",
    "  return cosine_similarities[:, 1:], indices[:, 1:]\n",
    "\n",
    "\n",
    "def numerical_preprocessing(data, features, n_neighbors=10):\n",
    "  normalized_data = normalize(data[features])\n",
    "\n",
    "  knn = NearestNeighbors(n_neighbors=n_neighbors+1, metric='euclidean')\n",
    "  knn.fit(normalized_data)\n",
    "  distances, indices = knn.kneighbors(normalized_data)\n",
    "  euclidean_similarities = 1 - distances\n",
    "\n",
    "  return euclidean_similarities[:, 1:], indices[:, 1:]\n",
    "\n",
    "\n",
    "def nominal_preprocessing(data, features, n_neighbors=10):\n",
    "  data = data[features].copy()\n",
    "  categorical = pd.concat([data['is_paid'].astype('uint8'), \n",
    "                           pd.get_dummies(data['category'], prefix='category', dtype='uint8'), \n",
    "                           pd.get_dummies(data['subcategory'], prefix='sub_category', dtype='uint8')],\n",
    "                           axis=1)\n",
    "\n",
    "  pca_result = PCA(n_components=0.95).fit_transform(categorical)\n",
    "  knn = NearestNeighbors(n_neighbors=n_neighbors+1, metric='cosine')\n",
    "  knn.fit(pca_result)\n",
    "  distances, indices = knn.kneighbors(pca_result)\n",
    "  cosine_similarities = 1 - distances\n",
    "\n",
    "  return cosine_similarities[:, 1:], indices[:, 1:]\n",
    "\n",
    "\n",
    "def ordinal_preprocessing(data, n_neighbors=10):\n",
    "  ordinal_data = data.rank(axis=0, method='average')\n",
    "\n",
    "  normalized_data = normalize(ordinal_data, norm='l2', axis=1)\n",
    "  nbrs = NearestNeighbors(n_neighbors=n_neighbors+1, metric='cosine')\n",
    "  nbrs.fit(normalized_data)\n",
    "\n",
    "  distances, indices = nbrs.kneighbors(normalized_data)\n",
    "  cosine_similarities = 1 - distances\n",
    "\n",
    "  return cosine_similarities[:, 1:], indices[:, 1:]\n",
    "\n",
    "\n",
    "def datetime_preprocessing(data, features, due='2022-10-10', decay=0.01, weights=None, n_neighbors=10):\n",
    "  data = data[features].copy()\n",
    "\n",
    "  for col in features:\n",
    "    data[col] = pd.to_datetime(data[col]).dt.tz_localize(None)\n",
    "    col_name = f\"{col}_duration\"\n",
    "    data[col_name] = (pd.to_datetime(due) - data[col]).dt.days\n",
    "    data[col_name] = data[col_name].apply(lambda x: np.exp(-decay * x))\n",
    "\n",
    "  decay_cols = [f\"{col}_duration\" for col in features]\n",
    "  date_matrix = data[decay_cols].values\n",
    "\n",
    "  if weights:\n",
    "      for i, col in enumerate(decay_cols):\n",
    "          date_matrix[:, i] *= weights.get(col, 1.0)\n",
    "\n",
    "  model = NearestNeighbors(n_neighbors=n_neighbors+1, metric='cosine')\n",
    "  model.fit(date_matrix)\n",
    "  distances, indices = model.kneighbors(date_matrix)\n",
    "  cosine_similarities = 1 - distances\n",
    "\n",
    "  return cosine_similarities[:, 1:], indices[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2b57e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_semantic, i_semantic = semantic_preprocessing(course_clean, types['semantic'])\n",
    "G_nominal, i_nominal = nominal_preprocessing(course_clean, types['nominal'])\n",
    "G_numeric, i_numeric = numerical_preprocessing(course_clean_scaled, features=numerical)\n",
    "G_ordinal, i_ordinal = ordinal_preprocessing(ordinal_mod)\n",
    "G_datetime, i_datetime= datetime_preprocessing(course_clean, types['datetime'])\n",
    "\n",
    "with open('preprocessed_data.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'G_semantic': G_semantic,\n",
    "        'i_semantic': i_semantic,\n",
    "        'G_nominal': G_nominal,\n",
    "        'i_nominal': i_nominal,\n",
    "        'G_numeric': G_numeric,\n",
    "        'i_numeric': i_numeric,\n",
    "        'ordinal_mod': ordinal_mod,\n",
    "        'G_ordinal': G_ordinal,\n",
    "        'i_ordinal': i_ordinal,\n",
    "        'G_datetime': G_datetime,\n",
    "        'i_datetime': i_datetime\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8c441904",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preprocessed_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "#similarities\n",
    "G_semantic = data['G_semantic']\n",
    "G_nominal = data['G_nominal']\n",
    "G_numeric = data['G_numeric']\n",
    "G_ordinal = data['G_ordinal']\n",
    "G_datetime = data['G_datetime']\n",
    "\n",
    "#indices \n",
    "i_semantic = data['i_semantic']\n",
    "i_nominal = data['i_nominal']\n",
    "i_numeric = data['i_numeric']\n",
    "i_ordinal = data['i_ordinal']\n",
    "i_datetime = data['i_datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "05553241",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = {\n",
    "    'semantic': (G_semantic, i_semantic),\n",
    "    'nominal': (G_nominal, i_nominal),\n",
    "    'ordinal': (G_numeric, i_numeric),\n",
    "    'datetime': (G_ordinal, i_ordinal),\n",
    "    'numeric': (G_datetime, i_datetime)\n",
    "}\n",
    "\n",
    "weights = {\n",
    "    'semantic': 0.4,\n",
    "    'nominal': 0.3,\n",
    "    'ordinal': 0.15,\n",
    "    'datetime': 0.10,\n",
    "    'numeric': 0.05\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6be0ffc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of final similarity matrix: (91566, 91566)\n",
      "Number of non-zero elements: 7939306\n"
     ]
    }
   ],
   "source": [
    "def aggregate_similarities(all_features, weights, num_items=len(course_clean), n_neighbors=10):\n",
    "    combined_scores = {}\n",
    "\n",
    "    for feature_type, (sim_matrix, idx_matrix) in all_features.items():\n",
    "        weight = weights.get(feature_type, 0)\n",
    "\n",
    "        for i in range(num_items):\n",
    "            for j in range(idx_matrix.shape[1]):\n",
    "                neighbor_idx = idx_matrix[i, j]\n",
    "                similarity_score = sim_matrix[i, j]\n",
    "                \n",
    "                key1 = (min(i, neighbor_idx), max(i, neighbor_idx))\n",
    "                \n",
    "                combined_scores[key1] = combined_scores.get(key1, 0.0) + (similarity_score * weight)\n",
    "\n",
    "    rows, cols, data = [], [], []\n",
    "    for (r, c), val in combined_scores.items():\n",
    "        rows.append(r)\n",
    "        cols.append(c)\n",
    "        data.append(val)\n",
    "\n",
    "        if r != c:\n",
    "            rows.append(c)\n",
    "            cols.append(r)\n",
    "            data.append(val)\n",
    "\n",
    "    final_similarity_matrix = csr_matrix((data, (rows, cols)), shape=(num_items, num_items))\n",
    "\n",
    "    return final_similarity_matrix\n",
    "\n",
    "\n",
    "final_sim_matrix = aggregate_similarities(all_features, weights)\n",
    "\n",
    "print(f\"Shape of final similarity matrix: {final_sim_matrix.shape}\")\n",
    "print(f\"Number of non-zero elements: {final_sim_matrix.nnz}\")\n",
    "\n",
    "with open('sparse_matrix.pkl', 'wb') as f:\n",
    "    pickle.dump(final_sim_matrix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6ed12da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_item_similarity_graph(similarity_matrix, threshold=0.5, top_n_edges_per_node=30):\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for i, item_id in enumerate(course_clean['id']):\n",
    "        G.add_node(i, item_id=item_id)\n",
    "\n",
    "    num_items = similarity_matrix.shape[0]\n",
    "\n",
    "    for i in range(num_items):\n",
    "        row_sims = similarity_matrix.getrow(i).toarray().flatten()\n",
    "        candidate_indices = np.where(row_sims > 0)[0] \n",
    "        candidate_indices = candidate_indices[candidate_indices != i] \n",
    "        \n",
    "        if len(candidate_indices) == 0:\n",
    "            continue\n",
    "\n",
    "        sorted_indices = candidate_indices[np.argsort(-row_sims[candidate_indices])]\n",
    "        \n",
    "        if top_n_edges_per_node is not None:\n",
    "            edges_to_add = sorted_indices[:top_n_edges_per_node]\n",
    "        else:\n",
    "            edges_to_add = sorted_indices \n",
    "\n",
    "        for j in edges_to_add:\n",
    "            sim_score = row_sims[j]\n",
    "            if sim_score > threshold:\n",
    "                if not G.has_edge(i, j):\n",
    "                    G.add_edge(i, j, weight=sim_score)\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fe38843e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in graph: 91566\n",
      "Number of edges in graph: 103668\n"
     ]
    }
   ],
   "source": [
    "G = build_item_similarity_graph(final_sim_matrix)\n",
    "\n",
    "print(f\"Number of nodes in graph: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges in graph: {G.number_of_edges()}\")\n",
    "\n",
    "with open('network_resource.pkl', 'wb') as g:\n",
    "     pickle.dump(G, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f014e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic tracing declarations\n",
    "def numerical_tracing(data, features):\n",
    "  normalized_data = normalize(data[features])\n",
    "  numerical_df = pd.DataFrame(normalized_data)\n",
    "\n",
    "  return numerical_df.describe().T\n",
    "\n",
    "def nominal_tracing(data, features):\n",
    "  categorical = pd.concat([data['is_paid'].astype('uint8'), \n",
    "                           pd.get_dummies(data['category'], prefix='category', dtype='uint8'), \n",
    "                           pd.get_dummies(data['subcategory'], prefix='sub_category', dtype='uint8')],\n",
    "                           axis=1)\n",
    "\n",
    "  pca_result = PCA(n_components=0.95).fit_transform(categorical)\n",
    "  pca_df = pd.DataFrame(pca_result, columns=[f'PC{i+1}' for i in range(pca_result.shape[1])])\n",
    "\n",
    "  return pca_df.describe().T\n",
    "\n",
    "\n",
    "def ordinal_tracing(data):\n",
    "  ordinal_data = data.rank(axis=0, method='average')\n",
    "  normalized_data = normalize(ordinal_data, norm='l2', axis=1)\n",
    "  ordinal_df = pd.DataFrame(normalized_data)\n",
    "\n",
    "  return ordinal_df.describe().T\n",
    "\n",
    "\n",
    "def datetime_tracing(data, features, due='2022-10-10', decay=0.01, weights=None):\n",
    "  for col in features:\n",
    "    data[col] = pd.to_datetime(data[col]).dt.tz_localize(None)\n",
    "    col_name = f\"{col}_duration\"\n",
    "    data[col_name] = (pd.to_datetime(due) - data[col]).dt.days\n",
    "    data[col_name] = data[col_name].apply(lambda x: np.exp(-decay * x))\n",
    "  decay_cols = [f\"{col}_duration\" for col in features]\n",
    "\n",
    "  return data[decay_cols].describe().T\n",
    "\n",
    "\n",
    "#basic tracing executions\n",
    "def tracing_all():\n",
    "    ordinal_mod = calc_smoothed_instructor_rating(course_clean_scaled, types['high_cardinal'])\n",
    "    ordinal = ordinal_tracing(ordinal_mod)\n",
    "    print('Ordinal Data')\n",
    "    print(ordinal.to_string())\n",
    "\n",
    "    numerics = numerical_tracing(course_clean_scaled, numerical)\n",
    "    print('\\nNumerical Data')\n",
    "    print(numerics.to_string())\n",
    "\n",
    "    nominal = nominal_tracing(course_clean_scaled, types['nominal'])\n",
    "    print('\\nNominal Data')\n",
    "    print(nominal.to_string())\n",
    "\n",
    "    datetime = datetime_tracing(course_clean_scaled, types['datetime'])\n",
    "    print('\\nDatetime Data')\n",
    "    print(datetime.to_string())  \n",
    "\n",
    "tracing_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f0f30909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity Statistics Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "      <th>25%</th>\n",
       "      <th>50% (Median)</th>\n",
       "      <th>75%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>semantic</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.545709</td>\n",
       "      <td>0.137603</td>\n",
       "      <td>0.443216</td>\n",
       "      <td>0.529759</td>\n",
       "      <td>0.632516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>numeric</th>\n",
       "      <td>3.424422e-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.880258</td>\n",
       "      <td>0.067472</td>\n",
       "      <td>0.842815</td>\n",
       "      <td>0.889016</td>\n",
       "      <td>0.925392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nominal</th>\n",
       "      <td>8.122366e-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.001075</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ordinal</th>\n",
       "      <td>9.978384e-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998198</td>\n",
       "      <td>0.042411</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Min  Max      Mean       Std       25%  50% (Median)  \\\n",
       "semantic  0.000000e+00  1.0  0.545709  0.137603  0.443216      0.529759   \n",
       "numeric   3.424422e-01  1.0  0.880258  0.067472  0.842815      0.889016   \n",
       "nominal   8.122366e-01  1.0  0.999994  0.001075  1.000000      1.000000   \n",
       "ordinal   9.978384e-01  1.0  1.000000  0.000012  1.000000      1.000000   \n",
       "datetime  2.220446e-16  1.0  0.998198  0.042411  1.000000      1.000000   \n",
       "\n",
       "               75%  \n",
       "semantic  0.632516  \n",
       "numeric   0.925392  \n",
       "nominal   1.000000  \n",
       "ordinal   1.000000  \n",
       "datetime  1.000000  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#similarity tracing\n",
    "def similarity_stats(sim_matrix):\n",
    "    flat_sim = sim_matrix.flatten()\n",
    "    stats = {\n",
    "        \"Min\": np.min(flat_sim),\n",
    "        \"Max\": np.max(flat_sim),\n",
    "        \"Mean\": np.mean(flat_sim),\n",
    "        \"Std\": np.std(flat_sim),\n",
    "        \"25%\": np.percentile(flat_sim, 25),\n",
    "        \"50% (Median)\": np.percentile(flat_sim, 50),\n",
    "        \"75%\": np.percentile(flat_sim, 75)}\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def similarity_tracing():\n",
    "    results = {}\n",
    "    results['semantic'] = similarity_stats(G_semantic)\n",
    "    results['numeric'] = similarity_stats(G_numeric)\n",
    "    results['nominal'] = similarity_stats(G_nominal)\n",
    "    results['ordinal'] = similarity_stats(G_ordinal)\n",
    "    results['datetime'] = similarity_stats(G_datetime)\n",
    "\n",
    "    df_stats = pd.DataFrame(results).T  # Transpose for readability\n",
    "    print(\"\\nSimilarity Statistics Summary:\")\n",
    "    \n",
    "    return df_stats\n",
    "\n",
    "similarity_tracing()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
